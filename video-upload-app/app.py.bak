import os
import base64
import logging
import time
import random
import requests
import traceback
from fastapi import FastAPI, File, UploadFile, HTTPException, Form, BackgroundTasks
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from moviepy.editor import VideoFileClip
from openai import OpenAI
from dotenv import load_dotenv
import instaloader
import glob
import shutil
from datetime import datetime, timedelta
import re
import json
from pathlib import Path
import tempfile
import argparse
from typing import List, Optional, Dict, Any, Union

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("app")

# Parse command line arguments
parser = argparse.ArgumentParser(description='Run the video transcription and fact checking API server')
parser.add_argument('--api-key', type=str, help='OpenAI API key (overrides .env file)')
parser.add_argument('--port', type=int, default=8000, help='Port to run the server on')
parser.add_argument('--host', type=str, default="127.0.0.1", help='Host to run the server on')
args, unknown = parser.parse_known_args()

# Load environment variables from .env file
env_path = os.getenv('ENV_FILE', os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env'))
load_dotenv(dotenv_path=env_path)

# Verify API key - command line takes precedence over .env
api_key = args.api_key or os.getenv('OPENAI_API_KEY')
if not api_key:
    raise ValueError("OPENAI_API_KEY environment variable is not set")

# Log a masked version of the key for debugging
masked_key = f"{api_key[:7]}...{api_key[-4:]}" if len(api_key) > 10 else "too_short"
logger.info(f"Using OpenAI API key starting with: {masked_key}")

# Initialize OpenAI client
client = OpenAI(api_key=api_key)

# ... (other imports and setup code)

INSTAGRAM_USERNAME = os.getenv('INSTAGRAM_USERNAME')
INSTAGRAM_PASSWORD = os.getenv('INSTAGRAM_PASSWORD')

# Get CORS settings from env with fallback to allow all origins in development
ALLOWED_ORIGINS_ENV = os.getenv('ALLOWED_ORIGINS', '*')
ALLOWED_ORIGINS = [origin.strip() for origin in ALLOWED_ORIGINS_ENV.split(',')] if ALLOWED_ORIGINS_ENV != '*' else ['*']

# Max retries for Instagram downloads
INSTAGRAM_MAX_RETRIES = int(os.getenv('INSTAGRAM_MAX_RETRIES', '3'))
INSTAGRAM_RETRY_DELAY = int(os.getenv('INSTAGRAM_RETRY_DELAY', '2'))

# Get model names from environment variables with defaults
FACT_CHECK_MODEL = os.getenv('FACT_CHECK_MODEL', 'gpt-4o-mini')
IMAGE_ANALYSIS_MODEL = os.getenv('IMAGE_ANALYSIS_MODEL', 'gpt-4o')
TRANSCRIPTION_MODEL = os.getenv('TRANSCRIPTION_MODEL', 'whisper-1')

# Web search configuration for fact checking
USE_WEB_SEARCH = os.getenv('USE_WEB_SEARCH', 'true').lower() in ('true', 'yes', '1')
WEB_SEARCH_MODEL = os.getenv('WEB_SEARCH_MODEL', 'gpt-4o')  # Fallback to standard gpt-4o
WEB_SEARCH_CONTEXT_SIZE = os.getenv('WEB_SEARCH_CONTEXT_SIZE', 'medium')

# Verify if web search model is properly configured
WEB_SEARCH_ENABLED_MODELS = ["gpt-4o-search-preview", "gpt-4o-mini-search-preview"]  # Models that support web search
FUNCTION_CALLING_MODELS = ["gpt-4o", "gpt-4", "gpt-3.5-turbo"]  # Models that support function calling

# If the specified model isn't in either list, fallback to gpt-4o
if WEB_SEARCH_MODEL not in WEB_SEARCH_ENABLED_MODELS and WEB_SEARCH_MODEL not in FUNCTION_CALLING_MODELS:
    logger.warning(f"Specified model {WEB_SEARCH_MODEL} may not support required features. Falling back to gpt-4o.")
    WEB_SEARCH_MODEL = "gpt-4o"

# Fact checking reliability settings
FACT_CHECK_MAX_RETRIES = int(os.getenv('FACT_CHECK_MAX_RETRIES', '3'))
FACT_CHECK_RETRY_DELAY = int(os.getenv('FACT_CHECK_RETRY_DELAY', '2'))
FACT_CHECK_TEMPERATURE = float(os.getenv('FACT_CHECK_TEMPERATURE', '0.2'))

# Instagram download method configuration
USE_YTDLP = os.getenv('USE_YTDLP', 'true').lower() in ('true', 'yes', '1')
USE_DIRECT_DOWNLOAD = os.getenv('USE_DIRECT_DOWNLOAD', 'true').lower() in ('true', 'yes', '1')
INSTAGRAM_DEBUG = os.getenv('INSTAGRAM_DEBUG', 'false').lower() in ('true', 'yes', '1')

app = FastAPI(root_path="/api", debug=True)
app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

UPLOAD_DIRECTORY = os.path.join(os.path.dirname(__file__), "uploads")
os.makedirs(UPLOAD_DIRECTORY, exist_ok=True)
os.chmod(UPLOAD_DIRECTORY, 0o755)

def cleanup_old_files():
    current_time = datetime.now()
    for filename in os.listdir(UPLOAD_DIRECTORY):
        file_path = os.path.join(UPLOAD_DIRECTORY, filename)
        if os.path.isfile(file_path):
            file_modified = datetime.fromtimestamp(os.path.getmtime(file_path))
            if current_time - file_modified > timedelta(hours=24):
                os.remove(file_path)
                logging.debug(f"Removed old file: {file_path}")

def perform_fact_check(transcript):
    logger.info("Starting fact-checking process")
    max_retries = FACT_CHECK_MAX_RETRIES
    retry_delay = FACT_CHECK_RETRY_DELAY
    
    # Using the specified model for fact checking
    model = WEB_SEARCH_MODEL if USE_WEB_SEARCH else FACT_CHECK_MODEL
    logger.info(f"Using model: {model} for fact checking")
    
    # Prepare the prompt
    verification_prompt = f"""
    The following is a transcript from a video. Please carefully analyze it to identify and fact-check key claims:
    
    TRANSCRIPT:
    {transcript}
    
    Instructions:
    1. Identify the 5-7 most significant factual claims from the transcript that can be verified.
    2. For each claim:
       a. Search for reliable sources to verify the claim.
       b. Rate accuracy on this scale: 
          - Accurate (claim fully supported by reliable sources)
          - Mostly True (claim mostly supported but with minor inaccuracies)
          - Partly Accurate (claim contains a mix of accurate and inaccurate elements)
          - Mostly False (claim contains more inaccuracies than accuracies)
          - False (claim contradicted by reliable sources)
          - Conspiracy Theory (claim represents well-known misinformation or conspiracy theory)
          - Unverified (insufficient reliable information available)
       c. Provide a brief explanation for your assessment with references to sources.
    3. Return your analysis as a JSON object with this structure:
    
    {{
        "claims": [
            {{
                "claim": "The exact claim from the transcript",
                "accuracy": "One of: accurate, mostly true, partly accurate, mostly false, false, conspiracy theory, unverified",
                "explanation": "Your explanation of why this claim is rated as such",
                "sources": ["List of sources if available"]
            }}
        ],
        "summary": "A brief summary of the overall truthfulness of the content"
    }}
    """
    
    # Make API request with retries
    for attempt in range(max_retries):
        try:
            logger.info(f"Fact check attempt {attempt+1}/{max_retries}")
            
            # Create the API request with appropriate parameters
            request_params = {
                "model": model,
                "messages": [
                    {"role": "system", "content": "You are a meticulous fact-checker specialized in verifying claims through web searches. Always prioritize accuracy and cite reliable sources. If you can't find information to verify a claim, state 'Unable to verify'. Never fabricate sources or citations."},
                    {"role": "user", "content": verification_prompt}
                ],
                "max_tokens": 4096,
                "temperature": FACT_CHECK_TEMPERATURE,
                "response_format": {"type": "json_object"}
            }
            
            # For web search models, we need to provide a properly formatted function
            if USE_WEB_SEARCH:
                # Only add tools parameter if the model supports it
                if WEB_SEARCH_MODEL in WEB_SEARCH_ENABLED_MODELS:
                    # Using a web search enabled model
                    logger.info(f"Using web search enabled model: {WEB_SEARCH_MODEL}")
                    # Define a search function for the model to use
                    request_params["tools"] = [
                        {
                            "type": "function",
                            "function": {
                                "name": "search_web",
                                "description": "Search the web for information to verify claims",
                                "parameters": {
                                    "type": "object",
                                    "properties": {
                                        "query": {
                                            "type": "string",
                                            "description": "The search query to find information about a claim"
                                        }
                                    },
                                    "required": ["query"]
                                }
                            }
                        }
                    ]
                    request_params["tool_choice"] = "auto"
                elif WEB_SEARCH_MODEL in FUNCTION_CALLING_MODELS:
                    # Using a model that supports function calling but not web search
                    logger.info(f"Using function calling model (without web search): {WEB_SEARCH_MODEL}")
                    # We'll directly ask the model to try to verify the claim with its knowledge
                else:
                    logger.warning(f"Model {WEB_SEARCH_MODEL} may not support all needed features")
            
            # Make the API call
            response = client.chat.completions.create(**request_params)
            fact_check_result = response.choices[0].message.content
            
            # Parse the result to verify format
            try:
                json_result = json.loads(fact_check_result)
                if "claims" in json_result and len(json_result["claims"]) > 0:
                    logger.info(f"Successfully fact-checked {len(json_result['claims'])} claims")
                    return json_result
                else:
                    logger.warning("API response missing required fields")
                    if attempt < max_retries - 1:
                        time.sleep(retry_delay)
                        continue
            except json.JSONDecodeError:
                logger.warning("Failed to parse JSON response")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    continue
                
        except Exception as e:
            logger.error(f"Error in fact-checking (attempt {attempt+1}/{max_retries}): {str(e)}")
            if attempt < max_retries - 1:
                logger.info(f"Retrying in {retry_delay} seconds...")
                time.sleep(retry_delay)
    
    # If all attempts failed
    logger.error("All fact-checking attempts failed")
    return {
        "claims": [
            {
                "claim": "Unable to perform fact-checking",
                "accuracy": "unverified",
                "explanation": "The fact-checking service encountered an error. Please try again later.",
                "sources": []
            }
        ],
        "summary": "Fact-checking failed due to technical errors."
    }

def analyze_image(file_path):
    # Load and encode the image
    try:
        with open(file_path, "rb") as image_file:
            base64_image = base64.b64encode(image_file.read()).decode('utf-8')
    except Exception as e:
        logger.error(f"Error loading image file: {str(e)}")
        return {"claims": [], "summary": f"Error loading image: {str(e)}"}

    # Step 1: Analyze the image with GPT-4o (non-search) to identify claims and generate questions
    try:
        logger.info("Step 1: Analyzing image with GPT-4o to identify claims")
        
        initial_analysis_prompt = f"""
        Analyze this image carefully and identify potential claims or elements that might need verification.
        
        For each significant claim or element in the image:
        1. Describe what is being shown or claimed
        2. Formulate a specific search query that would help verify this claim
        
        Return your analysis in this JSON format:
        {{
            "elements": [
                {{
                    "claim": "Description of what's shown or claimed in the image",
                    "search_query": "Specific search query to verify this claim"
                }}
            ],
            "summary": "Brief description of the overall image content"
        }}
        
        Identify at most 3 key elements that are most important to verify.
        """
        
        # First API call to analyze the image with GPT-4o
        initial_response = client.chat.completions.create(
            model=IMAGE_ANALYSIS_MODEL,  # Regular GPT-4o
            messages=[
                {"role": "system", "content": "You are an image analyzer that identifies claims and creates search queries to verify them."},
                {"role": "user", "content": [
                    {"type": "text", "text": initial_analysis_prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
                ]}
            ],
            max_tokens=1500,
            temperature=0.3,
            response_format={"type": "json_object"}
        )
        
        # Parse the initial analysis
        initial_analysis = json.loads(initial_response.choices[0].message.content)
        logger.info(f"Successfully identified {len(initial_analysis.get('elements', []))} elements to verify")
        
        # If no elements to verify, return basic result
        if not initial_analysis.get('elements'):
            return {
                "claims": [
                    {
                        "claim": "No verifiable claims identified in the image",
                        "accuracy": "unverified",
                        "explanation": "The system could not identify specific claims to verify in this image.",
                        "sources": []
                    }
                ],
                "summary": initial_analysis.get('summary', "No verifiable content identified")
            }
        
        # Step 2: Use web search to verify the identified claims if web search is enabled
        if USE_WEB_SEARCH:
            logger.info("Step 2: Using web search to verify claims")
            verified_claims = []
            
            for element in initial_analysis.get('elements', []):
                claim = element.get('claim')
                search_query = element.get('search_query')
                
                if not claim or not search_query:
                    continue
                
                # Create verification prompt for each claim
                verification_prompt = f"""
                Please verify this claim from an image: "{claim}"
                
                Use web search to find information about this claim. The suggested search query is: "{search_query}"
                
                After researching, provide:
                1. The accuracy of the claim (accurate, mostly true, partly accurate, misleading, false, conspiracy theory, or unverified)
                2. An explanation of your determination with cited sources
                3. List the specific sources you found
                
                Return the results in JSON format.
                """
                
                # Make the web search API call for this specific claim
                try:
                    logger.info(f"Verifying claim: {claim}")
                    
                    # Define the web search request
                    request_params = {
                        "model": WEB_SEARCH_MODEL,
                        "messages": [
                            {"role": "system", "content": "You are a meticulous fact-checker specialized in verifying claims through web searches."},
                            {"role": "user", "content": verification_prompt}
                        ],
                        "max_tokens": 1500,
                        "temperature": FACT_CHECK_TEMPERATURE,
                        "response_format": {"type": "json_object"}
                    }
                    
                    # Only add tools parameter if the model supports it
                    if WEB_SEARCH_MODEL in WEB_SEARCH_ENABLED_MODELS:
                        # Using a web search enabled model
                        logger.info(f"Using web search enabled model: {WEB_SEARCH_MODEL}")
                        request_params["tools"] = [
                            {
                                "type": "function",
                                "function": {
                                    "name": "search_web",
                                    "description": "Search the web for information to verify claims",
                                    "parameters": {
                                        "type": "object",
                                        "properties": {
                                            "query": {
                                                "type": "string",
                                                "description": "The search query to find information about a claim"
                                            }
                                        },
                                        "required": ["query"]
                                    }
                                }
                            }
                        ]
                        request_params["tool_choice"] = "auto"
                    elif WEB_SEARCH_MODEL in FUNCTION_CALLING_MODELS:
                        # Using a model that supports function calling but not web search
                        logger.info(f"Using function calling model (without web search): {WEB_SEARCH_MODEL}")
                        # We'll directly ask the model to try to verify the claim with its knowledge
                    else:
                        logger.warning(f"Model {WEB_SEARCH_MODEL} may not support all needed features")
                    
                    # Make the API call with retries
                    max_retries = 3
                    retry_delay = 2  # seconds
                    
                    for attempt in range(max_retries):
                        try:
                            response = client.chat.completions.create(**request_params)
                            claim_result = json.loads(response.choices[0].message.content)
                            
                            verified_claims.append({
                                "claim": claim,
                                "accuracy": claim_result.get("accuracy", "unverified"),
                                "explanation": claim_result.get("explanation", "No explanation provided"),
                                "sources": claim_result.get("sources", [])
                            })
                            
                            # Successfully verified this claim
                            break
                            
                        except Exception as e:
                            logger.error(f"Error verifying claim (attempt {attempt+1}/{max_retries}): {str(e)}")
                            if attempt < max_retries - 1:
                                logger.info(f"Retrying in {retry_delay} seconds...")
                                time.sleep(retry_delay)
                            else:
                                # Add unverified claim if all attempts fail
                                verified_claims.append({
                                    "claim": claim,
                                    "accuracy": "unverified",
                                    "explanation": f"Could not verify this claim: {str(e)}",
                                    "sources": []
                                })
                
                except Exception as e:
                    logger.error(f"Error setting up verification for claim: {str(e)}")
                    verified_claims.append({
                        "claim": claim,
                        "accuracy": "unverified",
                        "explanation": f"Error during verification process: {str(e)}",
                        "sources": []
                    })
            
            # Return the final result with verified claims
            return {
                "claims": verified_claims,
                "summary": initial_analysis.get('summary', "Analysis complete")
            }
        
        else:
            # If web search is not enabled, return initial analysis with unverified claims
            return {
                "claims": [
                    {
                        "claim": element.get('claim', ""),
                        "accuracy": "unverified",
                        "explanation": "Web search verification not enabled. This claim has not been verified.",
                        "sources": []
                    } for element in initial_analysis.get('elements', [])
                ],
                "summary": initial_analysis.get('summary', "Analysis complete without web verification")
            }
            
    except Exception as e:
        logger.error(f"Error in image analysis: {str(e)}")
        return {
            "claims": [
                {
                    "claim": "Error analyzing image",
                    "accuracy": "unverified",
                    "explanation": f"An error occurred during analysis: {str(e)}",
                    "sources": []
                }
            ],
            "summary": "Error processing image analysis"
        }

def process_video(video_path):
    try:
        audio_path = os.path.join(UPLOAD_DIRECTORY, "extracted_audio.wav")
        video = VideoFileClip(video_path)
        video.audio.write_audiofile(audio_path)
        video.close()

        with open(audio_path, "rb") as audio_file:
            transcription = client.audio.transcriptions.create(
                model=TRANSCRIPTION_MODEL, 
                file=audio_file
            )

        fact_check_result = perform_fact_check(transcription.text)

        os.remove(video_path)
        os.remove(audio_path)

        # Include model information in the response
        return JSONResponse(content={
            "transcription": transcription.text,
            "fact_check_result": fact_check_result,
            "models": {
                "transcription": TRANSCRIPTION_MODEL,
                "fact_check": FACT_CHECK_MODEL
            }
        })
    except Exception as e:
        logging.error(f"Error processing video: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing video: {str(e)}")

def download_instagram_video(url: str) -> str:
    """Download video from Instagram with better error handling and fallback mechanism"""
    is_docker = os.path.exists('/.dockerenv')  # Check if running in Docker
    
    if is_docker:
        logger.info("Running in Docker environment - using adapted Instagram download approach")
    
    # Try to extract shortcode from URL first
    shortcode = None
    if '/p/' in url:
        shortcode = url.split('/p/')[1].split('/')[0].split('?')[0]
    elif '/reel/' in url:
        shortcode = url.split('/reel/')[1].split('/')[0].split('?')[0]
    elif '/tv/' in url:
        shortcode = url.split('/tv/')[1].split('/')[0].split('?')[0]
    else:
        # Try to extract from end of URL
        path_parts = url.rstrip('/').split('/')
        if len(path_parts) > 0:
            potential_code = path_parts[-1].split('?')[0]
            if potential_code and not potential_code.startswith('http'):
                shortcode = potential_code
    
    if not shortcode:
        logger.error("Could not extract shortcode from Instagram URL")
        raise ValueError("Invalid Instagram URL format")
            
    logger.info(f"Extracted Instagram shortcode: {shortcode}")

    # First try direct method with yt-dlp if enabled and installed
    if USE_YTDLP and attempt_yt_dlp_download(url, shortcode):
        # Find downloaded media files
        media_files = find_media_files()
        
        if media_files:
            latest_media_file = max(media_files, key=os.path.getctime)
            logger.info(f"Found media file: {latest_media_file}")
            return latest_media_file
    
    # If yt-dlp fails or is disabled, fall back to instaloader with retries
    for attempt in range(INSTAGRAM_MAX_RETRIES):
        try:
            cleanup_old_files()
            logger.info(f"Instagram download attempt {attempt+1}/{INSTAGRAM_MAX_RETRIES} for URL: {url}")
            
            # Add jitter to delay to appear more like human behavior
            delay = INSTAGRAM_RETRY_DELAY + random.uniform(0.5, 2.0)
            logger.info(f"Waiting {delay:.2f} seconds before Instagram request")
            time.sleep(delay)
            
            # Setup instaloader with specific settings for Docker environment
            L = instaloader.Instaloader(
                dirname_pattern=UPLOAD_DIRECTORY,
                download_videos=True,
                download_video_thumbnails=False,
                download_geotags=False,
                download_comments=False,
                save_metadata=False,
                compress_json=False
            )
            
            # Track files before download to compare after
            files_before = set(os.listdir(UPLOAD_DIRECTORY))
            
            # Try loading session from file first if it exists
            try:
                session_file = os.path.join(os.path.dirname(__file__), "instagram_session")
                if os.path.exists(session_file):
                    logger.info("Found Instagram session file, attempting to load")
                    L.load_session_from_file(INSTAGRAM_USERNAME, session_file)
                    logger.info("Instagram session loaded successfully")
                    login_successful = True
                else:
                    login_successful = False
            except Exception as e:
                logger.error(f"Error loading Instagram session: {str(e)}")
            login_successful = False
            
            # Attempt login if session load failed
            if not login_successful and INSTAGRAM_USERNAME and INSTAGRAM_PASSWORD:
                try:
                    logger.info(f"Logging in to Instagram as {INSTAGRAM_USERNAME}")
                    L.login(INSTAGRAM_USERNAME, INSTAGRAM_PASSWORD)
                    logger.info("Instagram login successful")
                    # Save the session for future use
                    try:
                        L.save_session_to_file(session_file)
                        logger.info("Saved Instagram session for future use")
                    except Exception as e:
                        logger.error(f"Error saving Instagram session: {str(e)}")
                    login_successful = True
                    # Add substantial delay after login to reduce suspicion
                    time.sleep(3 if is_docker else 1.5)
                except Exception as login_error:
                    logger.error(f"Instagram login failed: {str(login_error)}")
                    logger.error(traceback.format_exc())
                    # Don't abort on login failure, try anonymous download or alternative method
                    
            # Try to download post with instaloader
            try:
                logger.info(f"Downloading Instagram post with shortcode: {shortcode}")
                post = instaloader.Post.from_shortcode(L.context, shortcode)
                L.download_post(post, target=UPLOAD_DIRECTORY)
                logger.info("Instagram download successful")
            except Exception as e:
                logger.error(f"Error downloading via Instaloader: {str(e)}")
                logger.error(traceback.format_exc())
                
                # More specific error details for debugging
                if "401" in str(e):
                    logger.error("Instagram 401 error: Authentication required or rate limited")
                elif "429" in str(e):
                    logger.error("Instagram 429 error: Too many requests, rate limited")
                elif "Login required" in str(e):
                    logger.error("Instagram requires login for this content")
                elif "window._sharedData" in str(e):
                    logger.error("Instagram page structure changed - parser needs updating")
                
                # Try alternative download method if instaloader fails and direct download is enabled
                if USE_DIRECT_DOWNLOAD and attempt_alternative_download(url, shortcode):
                    logger.info("Alternative download method succeeded")
                else:
                    raise e
            
            # Find newly added files by comparing directories before and after
            files_after = set(os.listdir(UPLOAD_DIRECTORY))
            new_files = files_after - files_before
            logger.info(f"New files after download: {new_files}")
            
            # First check specifically for video files
            video_files = [os.path.join(UPLOAD_DIRECTORY, f) for f in new_files 
                          if f.lower().endswith(('.mp4', '.mov', '.avi'))]
            
            if video_files:
                latest_video_file = max(video_files, key=lambda f: os.path.getctime(os.path.join(UPLOAD_DIRECTORY, os.path.basename(f))))
                logger.info(f"Found video file: {latest_video_file}")
                return latest_video_file
            
            # If no video files found but other files were added, check for images
            image_files = [os.path.join(UPLOAD_DIRECTORY, f) for f in new_files 
                          if f.lower().endswith(('.jpg', '.jpeg', '.png', '.webp'))]
            
            if image_files:
                latest_image_file = max(image_files, key=lambda f: os.path.getctime(os.path.join(UPLOAD_DIRECTORY, os.path.basename(f))))
                logger.info(f"Found image file: {latest_image_file}")
                return latest_image_file
                
            # If we've downloaded something but didn't catch it with the above logic, 
            # Try a broader media file search
            media_files = find_media_files()
            
            if media_files:
                latest_media_file = max(media_files, key=os.path.getctime)
                logger.info(f"Found media file through backup search: {latest_media_file}")
                return latest_media_file
            
            logger.warning("No media files found after download")
            raise FileNotFoundError("No media files downloaded")
            
        except Exception as e:
            logger.error(f"Instagram download attempt {attempt+1} failed: {str(e)}")
            
            # If we've tried all attempts, try the direct fallback approach
            if attempt == INSTAGRAM_MAX_RETRIES - 1:
                logger.warning("All Instagram download attempts failed, using fallback")
                return handle_instagram_fallback(url)
            
            # Otherwise wait before retrying with increasing delay
            retry_delay = INSTAGRAM_RETRY_DELAY * (attempt + 1) + random.uniform(1, 3)
            logger.info(f"Waiting {retry_delay:.2f} seconds before retry #{attempt+2}")
            time.sleep(retry_delay)
    
    # This should not be reached due to the fallback, but just in case
    raise HTTPException(
        status_code=500,
        detail="Failed to download Instagram content after multiple attempts"
    )

def find_media_files():
    """Helper function to find media files in the upload directory with more robust patterns"""
    # Look for all media files
    media_files = []
    
    # Look for standard media files
    for extension in ('.mp4', '.mov', '.avi', '.jpg', '.jpeg', '.png', '.gif', '.webp'):
        files = glob.glob(os.path.join(UPLOAD_DIRECTORY, f"*{extension}"))
        media_files.extend(files)
    
    # Look for date-formatted Instagram videos (YYYY-MM-DD_HH-MM-SS_UTC.mp4)
    date_pattern_files = glob.glob(os.path.join(UPLOAD_DIRECTORY, "[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]_*.mp4"))
    media_files.extend(date_pattern_files)
    
    # Only return recent files (created in the last 5 minutes)
    recent_media_files = [f for f in media_files if os.path.exists(f) and os.path.getmtime(f) > time.time() - 300]
    
    logger.info(f"Found {len(recent_media_files)} recent media files in upload directory")
    return recent_media_files

def attempt_yt_dlp_download(url: str, shortcode: str) -> bool:
    """Attempt to download using yt-dlp if available"""
    try:
        import subprocess
        
        output_template = os.path.join(UPLOAD_DIRECTORY, f"instagram_{shortcode}")
        
        # Check if yt-dlp is installed
        try:
            subprocess.run(["yt-dlp", "--version"], check=True, capture_output=True)
            ytdlp_installed = True
        except (subprocess.SubprocessError, FileNotFoundError):
            logger.warning("yt-dlp not installed, skipping this download method")
            return False
            
        if ytdlp_installed:
            logger.info("Attempting download with yt-dlp")
            
            # Run yt-dlp to download the video
            cmd = [
                "yt-dlp",
                "--no-warnings",
            ]
            
            # Add more verbose output if debug mode is enabled
            if INSTAGRAM_DEBUG:
                cmd.remove("--no-warnings")
                cmd.append("-v")
            else:
                cmd.append("--quiet")
                
            cmd.extend([
                "-o", f"{output_template}.%(ext)s",
                url
            ])
            
            logger.info(f"Running yt-dlp command: {' '.join(cmd)}")
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info("yt-dlp download successful")
                if INSTAGRAM_DEBUG and result.stdout:
                    logger.debug(f"yt-dlp output: {result.stdout}")
                return True
            else:
                logger.warning(f"yt-dlp download failed with code {result.returncode}")
                if result.stderr:
                    logger.warning(f"yt-dlp error: {result.stderr}")
                return False
    except Exception as e:
        logger.error(f"Error in yt-dlp download attempt: {str(e)}")
        logger.error(traceback.format_exc())
        return False
        
    return False

def attempt_alternative_download(url: str, shortcode: str) -> bool:
    """Alternative download method using direct API/requests approach"""
    try:
        # Use requests to get the video URL directly
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Pragma': 'no-cache',
            'Cache-Control': 'no-cache',
        }
        
        logger.info(f"Starting alternative download for Instagram URL: {url}")
        
        # Try to use session cookies from instaloader if available
        cookies = {}
        try:
            session_file = os.path.join(os.path.dirname(__file__), "instagram_session")
            if os.path.exists(session_file):
                logger.info("Attempting to extract cookies from Instagram session file")
                with open(session_file, 'r') as f:
                    session_content = f.read()
                    # Extract sessionid from session file
                    if 'sessionid' in session_content:
                        sessionid_match = re.search(r'"sessionid":\s*"([^"]+)"', session_content)
                        if sessionid_match:
                            cookies['sessionid'] = sessionid_match.group(1)
                            logger.info("Extracted sessionid from session file")
                    
                    # Extract csrf token from session file if available
                    if 'csrftoken' in session_content:
                        csrf_match = re.search(r'"csrftoken":\s*"([^"]+)"', session_content)
                        if csrf_match:
                            cookies['csrftoken'] = csrf_match.group(1)
                            headers['X-CSRFToken'] = csrf_match.group(1)
                            logger.info("Extracted csrftoken from session file")
                    
                    # Extract ds_user_id from session file if available
                    if 'ds_user_id' in session_content:
                        user_id_match = re.search(r'"ds_user_id":\s*"([^"]+)"', session_content)
                        if user_id_match:
                            cookies['ds_user_id'] = user_id_match.group(1)
                            logger.info("Extracted ds_user_id from session file")
        except Exception as e:
            logger.error(f"Error extracting cookies from session file: {str(e)}")
            # Continue without cookies
        
        # First try using embed URL which sometimes works without login
        embed_url = f"https://www.instagram.com/p/{shortcode}/embed/"
        logger.info(f"Attempting to fetch embed URL: {embed_url}")
        
        response = requests.get(embed_url, headers=headers, cookies=cookies, timeout=10)
        html_content = response.text
        
        if response.status_code != 200:
            # Fall back to original URL if embed fails
            logger.warning(f"Failed to get Instagram embed page: {response.status_code}")
            logger.info(f"Attempting to fetch main post URL: {url}")
            response = requests.get(url, headers=headers, cookies=cookies, timeout=10)
            html_content = response.text
            
            if response.status_code != 200:
                logger.warning(f"Failed to get Instagram page: {response.status_code}")
                return False
        
        # Save HTML for debugging if enabled
        if INSTAGRAM_DEBUG:
            debug_html_path = os.path.join(UPLOAD_DIRECTORY, f"instagram_debug_{shortcode}.html")
            with open(debug_html_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            logger.debug(f"Saved Instagram HTML for debugging to: {debug_html_path}")
        
        # Look for video URL patterns in the HTML
        video_url = extract_video_url(html_content)
                
        if not video_url:
            # If no video URL found, try using API if we have cookies
            if cookies and 'sessionid' in cookies:
                logger.info("Attempting to use Instagram API to get video URL")
                api_video_url = get_video_url_from_api(shortcode, cookies, headers)
                if api_video_url:
                    video_url = api_video_url
        
        if not video_url:
            # Final attempt - try the OEmbed API which sometimes works without auth
            oembed_url = f"https://api.instagram.com/oembed/?url=https://www.instagram.com/p/{shortcode}/"
            try:
                oembed_response = requests.get(oembed_url, headers=headers, timeout=10)
                if oembed_response.status_code == 200:
                    oembed_data = oembed_response.json()
                    # Extract URL from thumbnail URL by getting the post page
                    if 'thumbnail_url' in oembed_data:
                        logger.info(f"Found thumbnail URL in OEmbed response: {oembed_data['thumbnail_url']}")
                        # Use the thumbnail for now if we can't get the video
                        img_response = requests.get(oembed_data['thumbnail_url'], headers=headers, stream=True, timeout=30)
                        if img_response.status_code == 200:
                            output_path = os.path.join(UPLOAD_DIRECTORY, f"instagram_{shortcode}.jpg")
                            with open(output_path, 'wb') as f:
                                for chunk in img_response.iter_content(chunk_size=8192):
                                    f.write(chunk)
                            logger.info(f"Downloaded thumbnail image as fallback to: {output_path}")
                            return True
            except Exception as e:
                logger.error(f"Error fetching OEmbed data: {str(e)}")
        
        if not video_url:
            logger.warning("Could not extract video URL from Instagram page")
            return False
            
        # Download the video
        logger.info(f"Attempting to download video from URL: {video_url}")
        video_response = requests.get(video_url, headers=headers, stream=True, timeout=30)
        
        if video_response.status_code != 200:
            logger.warning(f"Failed to download video: {video_response.status_code}")
            return False
            
        # Save the video
        output_path = os.path.join(UPLOAD_DIRECTORY, f"instagram_{shortcode}.mp4")
        with open(output_path, 'wb') as f:
            for chunk in video_response.iter_content(chunk_size=8192):
                f.write(chunk)
                
        logger.info(f"Video downloaded successfully to {output_path}")
        return True
        
    except Exception as e:
        logger.error(f"Alternative download method failed: {str(e)}")
        logger.error(traceback.format_exc())
        return False

def extract_video_url(html_content):
    """Extract video URL from HTML content using various patterns"""
    import re
    
    # Pattern for video URL in the JSON data
    patterns = [
        r'"video_url":"([^"]*)"',  # Standard pattern
        r'property="og:video" content="([^"]*)"',  # Open Graph pattern
        r'"contentUrl": "([^"]*)"',  # JSON-LD pattern
        r'"contentUrl":"([^"]*)"',  # Alternative JSON-LD format
        r'video_url=([^&]*)',  # URL parameter pattern
        r'"video_versions":\[{"type":([^}]*)"url":"([^"]*)"',  # API response pattern
        r'<source src="([^"]*)" type="video/mp4">',  # HTML5 video tag
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, html_content)
        if matches:
            if isinstance(matches[0], tuple) and len(matches[0]) > 1:
                # Handle the case where the pattern has multiple capture groups
                video_url = matches[0][-1]  # Use the last group which should contain the URL
            else:
                video_url = matches[0]
                
            # Decode escaped JSON string
            video_url = video_url.replace('\\u0026', '&').replace('\\/', '/')
            logger.info(f"Found video URL using pattern: {pattern}")
            return video_url
            
    return None

def get_video_url_from_api(shortcode, cookies, headers):
    """Attempt to get video URL directly from Instagram API"""
    try:
        # First get the media ID from the shortcode
        media_id_url = f"https://www.instagram.com/p/{shortcode}/?__a=1&__d=dis"
        response = requests.get(media_id_url, headers=headers, cookies=cookies, timeout=10)
        
        if response.status_code != 200:
            logger.warning(f"Failed to get media ID: {response.status_code}")
            return None
            
        try:
            data = response.json()
            if 'items' in data and len(data['items']) > 0:
                media_id = data['items'][0].get('id')
                logger.info(f"Found media ID: {media_id}")
            else:
                # Try alternate JSON structure
                if 'graphql' in data and 'shortcode_media' in data['graphql']:
                    media_id = data['graphql']['shortcode_media'].get('id')
                    logger.info(f"Found media ID from graphql: {media_id}")
                else:
                    logger.warning("Could not find media ID in API response")
                    return None
        except Exception as e:
            logger.error(f"Error parsing media ID JSON: {str(e)}")
            return None
            
        # Now get the media info which includes video URLs
        info_url = f"https://i.instagram.com/api/v1/media/{media_id}/info/"
        info_response = requests.get(info_url, headers=headers, cookies=cookies, timeout=10)
        
        if info_response.status_code != 200:
            logger.warning(f"Failed to get media info: {info_response.status_code}")
            return None
            
        try:
            info_data = info_response.json()
            if 'items' in info_data and len(info_data['items']) > 0:
                item = info_data['items'][0]
                
                # Find video URL based on item type
                if 'video_versions' in item and len(item['video_versions']) > 0:
                    # Get the highest quality video
                    video_url = item['video_versions'][0]['url']
                    logger.info(f"Found video URL from API: {video_url}")
                    return video_url
                    
                # Check if this is a carousel and has videos
                if 'carousel_media' in item:
                    for carousel_item in item['carousel_media']:
                        if 'video_versions' in carousel_item and len(carousel_item['video_versions']) > 0:
                            video_url = carousel_item['video_versions'][0]['url']
                            logger.info(f"Found video URL from carousel: {video_url}")
                            return video_url
        except Exception as e:
            logger.error(f"Error parsing media info JSON: {str(e)}")
            
        return None
    except Exception as e:
        logger.error(f"Error getting video URL from API: {str(e)}")
        return None

def handle_instagram_fallback(url: str) -> str:
    """Fallback method when instaloader fails - returns error and suggests manual upload"""
    logger.info("Using Instagram fallback mechanism")
    
    # Create detailed error message
    error_message = (
        "Instagram download failed. Instagram's API is blocking automated access from our server. "
        "This is a common issue with Instagram's anti-scraping measures. "
        "Please download the video manually and upload it directly."
    )
    
    # Log the specific URL that failed
    logger.error(f"Instagram fallback triggered for URL: {url}")
    
    # Return a structured error response that the frontend can handle
    raise HTTPException(
        status_code=502,  # Bad Gateway
        detail={
            "error": "instagram_blocked",
            "message": error_message,
            "suggestion": "Please download the video manually and upload the file directly.",
            "url": url
        }
    )

@app.post("/upload")
async def upload_file(file: UploadFile = File(None), url: str = Form(None), background_tasks: BackgroundTasks = None):
    try:
        cleanup_old_files()
        
        if url:
            if "instagram.com" in url or "instagr.am" in url:
                try:
                    media_path = download_instagram_video(url)
                except HTTPException as e:
                    if e.status_code == 502 and isinstance(e.detail, dict) and e.detail.get("error") == "instagram_blocked":
                        # Return a more user-friendly error for frontend handling
                        return JSONResponse(
                            status_code=502,
                            content={
                                "error": "instagram_blocked",
                                "message": e.detail.get("message", "Instagram download failed"),
                                "suggestion": e.detail.get("suggestion", "Please upload the file directly"),
                                "url": e.detail.get("url", url)
                            }
                        )
                    else:
                        raise e
            else:
                # Handle other URLs if needed
                raise HTTPException(status_code=400, detail="Only Instagram URLs are currently supported")
        elif file:
            media_path = os.path.join(UPLOAD_DIRECTORY, file.filename)
            with open(media_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)
        else:
            raise HTTPException(status_code=400, detail="No file or URL provided")

        if not os.path.exists(media_path):
            raise HTTPException(status_code=404, detail=f"Media file not found: {media_path}")

        # Set up model information for response
        fact_check_model_info = {
            "name": WEB_SEARCH_MODEL if USE_WEB_SEARCH else FACT_CHECK_MODEL,
            "type": "Web Search Enabled" if USE_WEB_SEARCH else "Standard"
        }
        
        if USE_WEB_SEARCH:
            fact_check_model_info["context_size"] = WEB_SEARCH_CONTEXT_SIZE

        models_info = {
            "transcription": {
                "name": TRANSCRIPTION_MODEL,
                "type": "Audio Transcription"
            },
            "fact_check": fact_check_model_info,
            "image_analysis": {
                "name": IMAGE_ANALYSIS_MODEL,
                "type": "Vision Analysis",
                "web_search": {
                    "enabled": USE_WEB_SEARCH,
                    "model": WEB_SEARCH_MODEL if USE_WEB_SEARCH else None,
                    "context_size": WEB_SEARCH_CONTEXT_SIZE if USE_WEB_SEARCH else None
                }
            }
        }

        if media_path.lower().endswith(('.mp4', '.mov', '.avi')):
            result = process_video(media_path)
            # Process_video returns a dict with transcription and fact_check
            # Add the models info to it
            if isinstance(result, dict):
                result["models"] = models_info
            return result
        elif media_path.lower().endswith(('.jpg', '.jpeg', '.png', '.gif')):
            image_analysis = analyze_image(media_path)
            return JSONResponse(content={
                "image_analysis": image_analysis,
                "models": models_info
            })
        else:
            raise HTTPException(status_code=400, detail=f"Unsupported media type: {media_path}")

    except HTTPException as he:
        raise he
    except Exception as e:
        logger.error(f"Error processing upload: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models")
def get_models():
    """Return information about the AI models used in the application"""
    fact_check_model_info = {
        "name": FACT_CHECK_MODEL,
        "type": "Web Search Enabled" if USE_WEB_SEARCH else "Standard"
    }
    
    if USE_WEB_SEARCH:
        fact_check_model_info["search_model"] = WEB_SEARCH_MODEL
        fact_check_model_info["context_size"] = WEB_SEARCH_CONTEXT_SIZE
    
    return {
        "transcription": {
            "name": TRANSCRIPTION_MODEL,
            "type": "Audio Transcription"
        },
        "fact_check": fact_check_model_info,
        "image_analysis": {
            "name": IMAGE_ANALYSIS_MODEL,
            "type": "Vision Analysis"
        }
    }

@app.post("/analyze-image")
async def upload_image(image: UploadFile = File(...)):
    logger.info(f"Received image upload: {image.filename}")
    
    # Create directory if it doesn't exist
    os.makedirs(UPLOAD_DIRECTORY, exist_ok=True)
    
    # Save the uploaded image
    file_path = os.path.join(UPLOAD_DIRECTORY, image.filename)
    with open(file_path, "wb") as buffer:
        buffer.write(await image.read())
    
    logger.info(f"Image saved to {file_path}")
    
    # Analyze the image
    analysis_result = analyze_image(file_path)
    
    # Parse the JSON result
    try:
        if isinstance(analysis_result, str):
            analysis_data = json.loads(analysis_result)
        else:
            analysis_data = analysis_result
    except json.JSONDecodeError:
        analysis_data = {
            "claims": [
                {
                    "claim": "Error processing analysis result",
                    "accuracy": "unverified",
                    "explanation": "The analysis result could not be properly parsed.",
                    "sources": []
                }
            ],
            "summary": "An error occurred processing the analysis result."
        }
    
    # Return analysis results
    return JSONResponse(content={
        "image_analysis": analysis_data,
        "models": {
            "analysis": IMAGE_ANALYSIS_MODEL,
            "web_search": WEB_SEARCH_MODEL if USE_WEB_SEARCH else "Not used"
        }
    })

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host=args.host, port=args.port)